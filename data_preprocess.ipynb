{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim import corpora, models\n",
    "from gensim.similarities import MatrixSimilarity\n",
    "from gensim.utils import SaveLoad\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re \n",
    "import pyLDAvis.gensim\n",
    "from collections import Counter\n",
    "from gensim.matutils import corpus2csc, sparse2full, corpus2dense\n",
    "from wordcloud import WordCloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "job_data = pd.read_csv('./data/SF_jobs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "job_data.drop('Unnamed: 0',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Functions for data cleaning\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def text_cleaner(text):\n",
    "    \n",
    "    text = text.lower().split()\n",
    "    text = [wnl.lemmatize(myWord) for myWord in text] # Lemmertization\n",
    "\n",
    "    stop_words = set(stopwords.words(\"english\")) # Filter out any stop words\n",
    "    text = [w for w in text if not w in stop_words]\n",
    "    \n",
    "    return text # return list of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cleanDescription = []\n",
    "for i in range(1500):\n",
    "    cleanDescription.append(text_cleaner(job_data.job_description[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeDict(myTweetList):\n",
    "    \"\"\"Create dictionary from list of tokenized documents\"\"\"\n",
    "    return corpora.Dictionary(myTweetList)\n",
    "\n",
    "def makeCorpus(myTweetList,myDict):\n",
    "    \"\"\"Create corpus from list of tokenized documents\"\"\"\n",
    "    return [myDict.doc2bow(tweet) for tweet in myTweetList]\n",
    "\n",
    "def createLDA(myCorpus, myDictionary,myTopics=10,myPasses=10,myIterations=50,myAlpha=0.001):\n",
    "    \"\"\"LDA model call function\"\"\"\n",
    "    return models.LdaMulticore(myCorpus, id2word=myDictionary, num_topics=myTopics, passes=myPasses,\n",
    "    iterations=myIterations,alpha=myAlpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kagDict   = makeDict(cleanDescription)\n",
    "kagCorpus = makeCorpus(cleanDescription, kagDict)\n",
    "kagLda = createLDA(kagCorpus, kagDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "ldaViz = pyLDAvis.gensim.prepare(kagLda, kagCorpus, kagDict)\n",
    "ldaViz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
